# -*- coding: utf-8 -*-
"""metrics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_X4nAqeOnDW75jRCmWnnZH3eIw83XnD1
"""

import torch
import torch.nn.functional as F
import numpy as np
from scipy.ndimage import distance_transform_edt, binary_erosion


# ------------------------------
# Binary Classification Metrics
# ------------------------------
def binary_classification_metrics(preds, targets, epsilon=1e-6):
    preds = preds.view(preds.size(0), -1)
    targets = targets.view(targets.size(0), -1)

    TP = ((preds == 1) & (targets == 1)).sum(dim=1).float()
    TN = ((preds == 0) & (targets == 0)).sum(dim=1).float()
    FP = ((preds == 1) & (targets == 0)).sum(dim=1).float()
    FN = ((preds == 0) & (targets == 1)).sum(dim=1).float()

    dice = (2 * TP + epsilon) / (2 * TP + FP + FN + epsilon)
    specificity = (TN + epsilon) / (TN + FP + epsilon)
    sensitivity = (TP + epsilon) / (TP + FN + epsilon)
    precision = (TP + epsilon) / (TP + FP + epsilon)
    f1 = (2 * precision * sensitivity) / (precision + sensitivity + epsilon)

    return {
        "Dice": dice.mean().item(),
        "Specificity": specificity.mean().item(),
        "Sensitivity": sensitivity.mean().item(),
        "Precision": precision.mean().item(),
        "F1": f1.mean().item()
    }


# ------------------------------
# One-hot Encode Helper
# ------------------------------
def one_hot_encode(labels, num_classes):
    return F.one_hot(labels, num_classes).permute(0, 3, 1, 2).float()


# ------------------------------
# Multi-class Dice
# ------------------------------
def multiclass_dice(preds, targets, epsilon=1e-6):
    # preds: [B, C, H, W], targets: [B, H, W]
    num_classes = preds.shape[1]
    targets_one_hot = one_hot_encode(targets, num_classes).to(preds.device)

    preds = F.softmax(preds, dim=1)

    dice_scores = []
    for c in range(num_classes):
        pred_flat = preds[:, c].contiguous().view(-1)
        target_flat = targets_one_hot[:, c].contiguous().view(-1)

        intersection = (pred_flat * target_flat).sum()
        dice = (2. * intersection + epsilon) / (pred_flat.sum() + target_flat.sum() + epsilon)
        dice_scores.append(dice)

    return torch.stack(dice_scores).mean().item()


# ------------------------------
# Normalized Multi-class Dice
# ------------------------------
def normalized_multiclass_dice(preds, targets, epsilon=1e-6):
    num_classes = preds.shape[1]
    targets_one_hot = F.one_hot(targets, num_classes).permute(0, 3, 1, 2).float().to(preds.device)
    preds = F.softmax(preds, dim=1)

    dice_scores = []
    class_weights = []

    for c in range(num_classes):
        pred_flat = preds[:, c].contiguous().view(-1)
        target_flat = targets_one_hot[:, c].contiguous().view(-1)

        intersection = (pred_flat * target_flat).sum()
        dsc = (2. * intersection + epsilon) / (pred_flat.sum() + target_flat.sum() + epsilon)
        dice_scores.append(dsc)

        weight = 1.0 / (target_flat.sum() + epsilon)
        class_weights.append(weight)

    class_weights = torch.stack(class_weights)
    class_weights = class_weights / class_weights.sum()

    nDSC = torch.stack(dice_scores) * class_weights
    return nDSC.sum().item()


# ------------------------------
# Normalized Surface Dice (NSD)
# ------------------------------
def surface_dice(pred_mask, gt_mask, tolerance=1.0):
    """
    pred_mask, gt_mask: numpy arrays (H, W) or (D, H, W), binary {0,1}
    tolerance: distance in pixels
    """
    pred_mask = pred_mask.astype(bool)
    gt_mask = gt_mask.astype(bool)

    pred_surface = pred_mask ^ binary_erosion(pred_mask)
    gt_surface = gt_mask ^ binary_erosion(gt_mask)

    dt_pred = distance_transform_edt(~pred_mask)
    dt_gt = distance_transform_edt(~gt_mask)

    gt_close = dt_pred[gt_surface] <= tolerance
    pred_close = dt_gt[pred_surface] <= tolerance

    nsd = (np.sum(gt_close) + np.sum(pred_close)) / (np.sum(pred_surface) + np.sum(gt_surface) + 1e-6)
    return nsd


# ------------------------------
# Specificity
# ------------------------------
def specificity_score(preds, targets, epsilon=1e-6):
    preds = preds.view(preds.size(0), -1)
    targets = targets.view(targets.size(0), -1)

    TN = ((preds == 0) & (targets == 0)).sum(dim=1).float()
    FP = ((preds == 1) & (targets == 0)).sum(dim=1).float()

    specificity = (TN + epsilon) / (TN + FP + epsilon)
    return specificity.mean().item()


# ------------------------------
# Tumor Size Calculations
# ------------------------------
def compute_tumor_size(mask, spacing=(1.0, 1.0, 1.0)):
    """
    Compute tumor size from segmentation mask.

    Args:
        mask: numpy array (2D or 3D), binary {0,1}
        spacing: tuple of voxel spacing (dx, dy) for 2D, (dx, dy, dz) for 3D in mm

    Returns:
        dict with pixel_count/voxel_count and area_cm2 or volume_cm3
    """
    mask = mask.astype(bool)
    pixel_count = mask.sum()

    if mask.ndim == 2:
        dx, dy = spacing
        area_mm2 = pixel_count * dx * dy
        area_cm2 = area_mm2 / 100.0
        return {
            "pixel_count": int(pixel_count),
            "area_cm2": area_cm2
        }

    elif mask.ndim == 3:
        dx, dy, dz = spacing
        volume_mm3 = pixel_count * dx * dy * dz
        volume_cm3 = volume_mm3 / 1000.0
        return {
            "voxel_count": int(pixel_count),
            "volume_cm3": volume_cm3
        }

    else:
        raise ValueError("Mask must be 2D or 3D numpy array.")